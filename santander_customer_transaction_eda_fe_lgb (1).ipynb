{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "15af321ef7cb2dc51b6b1dbcd1b74e85d834db50",
    "id": "P7914IXEgNeG"
   },
   "source": [
    "## **0. Introduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "B41GCCIagNeM"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.display.max_rows = 200\n",
    "pd.options.display.max_columns = 200\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(style=\"darkgrid\")\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import OneHotEncoder, MinMaxScaler, StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import lightgbm as lgb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ml6IYaHegVig"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ywy4P0S_BFyq"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IWxRYB4bBP4C"
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "!unzip '/content/drive/MyDrive/santander-customer-transaction-prediction (1).zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLKcsfLdgVjt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "id": "5RCNzKs7gNeN"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_train.name = 'Training Set'\n",
    "df_test = pd.read_csv('test.csv')\n",
    "df_test.name = 'Test Set'\n",
    "\n",
    "print('Number of Training Examples = {}'.format(df_train.shape[0]))\n",
    "print('Number of Test Examples = {}'.format(df_test.shape[0]))\n",
    "print('Training X Shape = {}'.format(df_train.shape))\n",
    "print('Training y Shape = {}'.format(df_train['target'].shape[0]))\n",
    "print('Test X Shape = {}'.format(df_test.shape))\n",
    "print('Test y Shape = {}'.format(df_test.shape[0]))\n",
    "print(df_train.columns)\n",
    "print(df_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "bc381219289b54ea1cbd3c5fa2e5b3042c0abefa",
    "id": "5hCLygIWgNeN"
   },
   "source": [
    "## **1. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "df9afedc0ac107e083ef47e9ccb3bdf55d75c336",
    "id": "_lOAzEIlgNeO"
   },
   "source": [
    "### **1.1 Overview**\n",
    "* Both training set and test set have **200000** rows\n",
    "* Training set have **202** features and test set have **201** features\n",
    "* One extra feature in the training set is `target` feature, which is the class of a row\n",
    "* `target` feature is binary (**0** or **1**), **1 = transaction** and **0 = no transaction**\n",
    "* `ID_code` feature is the unique id of the row and it doesn't have any effect on target\n",
    "* The other features are anonymized and labeled from `var_0` to `var_199`\n",
    "* There are no missing values in both training set and test set because the dataset is already processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "80ef7466ba79b96e1486d80b21d3b349ca648e58",
    "id": "MyVXBlKigNeO"
   },
   "outputs": [],
   "source": [
    "print(df_train.info())\n",
    "df_train.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e33ddc4e9896058483da6b62870dde051eff093",
    "id": "OslRkFlMgNeO"
   },
   "outputs": [],
   "source": [
    "print(df_test.info())\n",
    "df_test.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "377a25bf66ed25b2238a3a6a5deb2cef7efecb98",
    "id": "ZAq0DM2NgNeO"
   },
   "source": [
    "### **1.2 Target Distribution**\n",
    "* **10.05%** (20098/200000) of the training set is **Class 1**\n",
    "* **89.95%** (179902/200000) of the training set is **Class 0**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7d5f6be7f849ef5e7c83be6e6a901d1b738c4213",
    "id": "tLUeRRSEgNeP"
   },
   "outputs": [],
   "source": [
    "ones = df_train['target'].value_counts()[1]\n",
    "zeros = df_train['target'].value_counts()[0]\n",
    "ones_per = ones / df_train.shape[0] * 100\n",
    "zeros_per = zeros / df_train.shape[0] * 100\n",
    "\n",
    "print('{} out of {} rows are Class 1 and it is the {:.2f}% of the dataset.'.format(ones, df_train.shape[0], ones_per))\n",
    "print('{} out of {} rows are Class 0 and it is the {:.2f}% of the dataset.'.format(zeros, df_train.shape[0], zeros_per))\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.countplot(df_train['target'])\n",
    "\n",
    "plt.xlabel('Target')\n",
    "plt.xticks((0, 1), ['Class 0 ({0:.2f}%)'.format(ones_per), 'Class 1 ({0:.2f}%)'.format(zeros_per)])\n",
    "plt.ylabel('Count')\n",
    "plt.title('Training Set Target Distribution')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ed545b07baedf94e2b575b4317758299c113cd92",
    "id": "-TqdK4cKgNeP"
   },
   "source": [
    "### **1.3 Correlations**\n",
    "Features from `var_0` to `var_199` have extremely low correlation between each other in both training set and test set. The lowest correlation between variables is **2.7e-8** and it is in the training set (between `var_191` and `var_75`). The highest correlation between variables is **0.00986** and it is in the test set (between `var_139` and `var_75`). `target` has slightly higher correlations with other features. The highest correlation between a feature and `target` is **0.08** (between `var_81` and `target`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OgxKwe8kgNeP"
   },
   "outputs": [],
   "source": [
    "df_train_corr = df_train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "df_train_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_train_corr.drop(df_train_corr.iloc[1::2].index, inplace=True)\n",
    "df_train_corr_nd = df_train_corr.drop(df_train_corr[df_train_corr['Correlation Coefficient'] == 1.0].index)\n",
    "\n",
    "df_test_corr = df_test.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\n",
    "df_test_corr.rename(columns={\"level_0\": \"Feature 1\", \"level_1\": \"Feature 2\", 0: 'Correlation Coefficient'}, inplace=True)\n",
    "df_test_corr.drop(df_test_corr.iloc[1::2].index, inplace=True)\n",
    "df_test_corr_nd = df_test_corr.drop(df_test_corr[df_test_corr['Correlation Coefficient'] == 1.0].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JY_ZJEKhgNeQ"
   },
   "outputs": [],
   "source": [
    "# Top 5 Highest Correlations in the Training Set\n",
    "df_train_corr_nd.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adf8KhWZgNeQ"
   },
   "outputs": [],
   "source": [
    "# Top 5 Highest Correlations between variables in the Training Set\n",
    "df_train_corr_nd[np.logical_and(df_train_corr_nd['Feature 1'] != 'target', df_train_corr_nd['Feature 2'] != 'target')].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_z0xnnBngNeQ"
   },
   "outputs": [],
   "source": [
    "# Top 5 Highest Correlations in the Test Set\n",
    "df_test_corr_nd.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4a04be36b718111f037b6608cea4838c57b188c6",
    "id": "ZAiUYg1NgNeQ"
   },
   "source": [
    "### **1.4 Unique Value Count**\n",
    "The lowest unique value count belongs to `var_68` which has only **451** unique values in training set and **428** unique values in test set. **451** and **428** unique values in **200000** rows are too less that `var_68` could even be a categorical feature. The highest unique value count belongs to`var_45` which has **169968** unique values in the training set and **92058** unique values in the test set. Every feature in training set have higher unique value counts compared to features in test set.\n",
    "\n",
    "The lowest unique value count difference is in the `var_68` feature (Training Set Unique Count **451**, Test Set Unique Count **428**). The highest unique value count difference is in the `var_45` feature (Training Set Unique Count **169968**, Test Set Unique Count **92058**). When the unique value count of a feature increases, the difference between training set unique value count and test set unique value count also increases. The explanation of this situation is probably the synthetic records in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j8D1QLSWgNeR"
   },
   "outputs": [],
   "source": [
    "df_train_unique = df_train.agg(['nunique']).transpose().sort_values(by='nunique')\n",
    "df_test_unique = df_test.agg(['nunique']).transpose().sort_values(by='nunique')\n",
    "df_uniques = df_train_unique.drop('target').reset_index().merge(df_test_unique.reset_index(), how='left', right_index=True, left_index=True)\n",
    "df_uniques.drop(columns=['index_y'], inplace=True)\n",
    "df_uniques.columns = ['Feature', 'Training Set Unique Count', 'Test Set Unique Count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4409e3b81cc1f82f8024389d7a4988dad0f5efc1",
    "id": "mxGj0IKvgNeR"
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=(24, 12))\n",
    "\n",
    "sns.barplot(x=df_train_unique.index[1:6], y=\"nunique\", data=df_train_unique[1:].head(), ax=axs[0][0])\n",
    "sns.barplot(x=df_test_unique.index[:5], y=\"nunique\", data=df_test_unique.head(), ax=axs[0][1])\n",
    "sns.barplot(x=df_train_unique.index[-6:-1], y=\"nunique\", data=df_train_unique[-6:-1].tail(), ax=axs[1][0])\n",
    "sns.barplot(x=df_test_unique.index[-6:-1], y=\"nunique\", data=df_test_unique[-6:-1].tail(), ax=axs[1][1])\n",
    "\n",
    "for i in range(2):\n",
    "    for j in range(2):        \n",
    "        axs[i][j].set(xlabel='Features', ylabel='Unique Count')\n",
    "        \n",
    "axs[0][0].set_title('Training Set Features with Least Unique Values')\n",
    "axs[0][1].set_title('Test Set Features with Least Unique Values')\n",
    "axs[1][0].set_title('Training Set Features with Most Unique Values')\n",
    "axs[1][1].set_title('Test Set Features with Most Unique Values')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MKhYZrSigNeS"
   },
   "source": [
    "### **1.5 Target Distribution in Quartiles**\n",
    "Class 1 `target` distribution in feature quartiles are quite similar for each feature. Most of the class 1 `target` rows are either in the **1st** quartile or in the **4th** quartile of the features because of the winsorization. Winsorization clips the extreme values, so they are grouped up in the spikes inside **1st** quartile and **4th** quartile.\n",
    "* **94** features have highest class 1 `target` percentage in **1st** quartile\n",
    "* **101** features have highest class 1 `target` percentage in **4th** quartile\n",
    "* Only **5** features have highest class 1 `target` percetange in **2nd** and **3rd** quartile, and those features are `var_17`, `var_30`, `var_100`, `var_101`, `var_105`\n",
    "\n",
    "Maximum class 1 `target` percentage for **1st** quartile is **14.35%** (**85.65%** class 0), and for **4th** quartile is **13.43%** (**86.57%** class 0). Maximum class 1 `target` percentage for **2nd** quartile is **10.34%** (**89.66%** class 0), and for **3rd** quartile is **10.05%** (**89.95%** class 0 `target`). To conclude, values in **1st** and **4th** quartiles have higher chance (**3-4%**) to be class 1 than values in **2nd** and **3rd** quartile for 195 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dryrlO4egNeS"
   },
   "outputs": [],
   "source": [
    "df_qdist = pd.DataFrame(np.zeros((200, 9)), columns=['Quartile 1 Positives', 'Quartile 2 Positives', 'Quartile 3 Positives', 'Quartile 4 Positives',\n",
    "                                                     'Quartile 1 Positive Percentage', 'Quartile 2 Positive Percentage', 'Quartile 3 Positive Percentage', 'Quartile 4 Positive Percentage',\n",
    "                                                     'Quartile Order'])\n",
    "features = [col for col in df_train.columns.values.tolist() if col.startswith('var')]\n",
    "quartiles = np.arange(0, 1, 0.25)\n",
    "df_qdist.index = features\n",
    "\n",
    "for i, feature in enumerate(features):\n",
    "    for j, quartile in enumerate(quartiles):\n",
    "        target_counts = df_train[np.logical_and(df_train[feature] >= df_train[feature].quantile(q=quartile), \n",
    "                                                df_train[feature] < df_train[feature].quantile(q=quartile + 0.25))].target.value_counts()\n",
    "        \n",
    "        ones_per = target_counts[1] / (target_counts[0] + target_counts[1]) * 100\n",
    "        df_qdist.iloc[i, j] = target_counts[1]\n",
    "        df_qdist.iloc[i, j + 4] = ones_per\n",
    "\n",
    "pers = df_qdist.columns.tolist()[4:-1]         \n",
    "        \n",
    "for i, index in enumerate(df_qdist.index):\n",
    "    order = df_qdist[pers].iloc[[i]].sort_values(by=index, ascending=False, axis=1).columns\n",
    "    order_str = ''.join([col[9] for col in order])\n",
    "    df_qdist.iloc[i, 8] = order_str        \n",
    "                \n",
    "df_qdist = df_qdist.round(2)\n",
    "df_qdist.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3L1HeXItgNeS"
   },
   "outputs": [],
   "source": [
    "# 5 features that doesn't have highest positive target percentage in 1st and 4th quartiles\n",
    "df_qdist[np.logical_or(df_qdist['Quartile Order'].str.startswith('2'), df_qdist['Quartile Order'].str.startswith('3'))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "hEEXe9degNeT"
   },
   "outputs": [],
   "source": [
    "for i, col in enumerate(pers):    \n",
    "    print('There are {} features that have the highest positive target percentage in Quartile {}'.format(df_qdist[df_qdist['Quartile Order'].str.startswith(str(i + 1))].count()[0],\n",
    "                                                                                                            i + 1))\n",
    "    print('Quartile {} max positive target percentage = {}% ({})'.format(i + 1, df_qdist[col].max(), df_qdist[col].argmax()))\n",
    "    print('Quartile {} min positive target percentage = {}% ({})\\n'.format(i + 1, df_qdist[col].min(), df_qdist[col].argmin()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4LBV9ECfgNeT"
   },
   "source": [
    "### **1.6 Feature Distributions in Training and Test Set**\n",
    "Training and test set distributions of features are not perfectly identical. There are bumps on the distribution peaks of test set because the unique value counts are lesser than training set. Distribution tails are smoother than peaks and spikes are present in both training and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "MfyQ-MpDgNeT"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns.tolist() if col.startswith('var')]\n",
    "\n",
    "nrows = 50\n",
    "fig, axs = plt.subplots(nrows=50, ncols=4, figsize=(24, nrows * 5))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(50, 4, i)\n",
    "    sns.kdeplot(df_train[feature], bw='silverman', label='Training Set', shade=True)\n",
    "    sns.kdeplot(df_test[feature], bw='silverman', label='Test Set', shade=True)\n",
    "    \n",
    "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=8)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title('Distribution of {} in Training and Test Set'.format(feature))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d487V3-kgNeT"
   },
   "source": [
    "### **1.7 Target Distributions in Features**\n",
    "Majority of the features have good split points and huge spikes. This explains why a simple LightGBM model can achieve 0.90 AUC. Distribution difference is bigger in tails because of winsorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "JXzlfUUYgNeU"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns.tolist() if col.startswith('var')]\n",
    "\n",
    "nrows = 50\n",
    "fig, axs = plt.subplots(nrows=50, ncols=4, figsize=(24, nrows * 5))\n",
    "\n",
    "for i, feature in enumerate(features, 1):\n",
    "    plt.subplot(50, 4, i)\n",
    "    \n",
    "    sns.distplot(StandardScaler().fit_transform(df_train[df_train['target'] == 0][feature].values.reshape(-1, 1)), label='Target=0', hist=True, color='#e74c3c')\n",
    "    sns.distplot(StandardScaler().fit_transform(df_train[df_train['target'] == 1][feature].values.reshape(-1, 1)), label='Target=1', hist=True, color='#2ecc71')\n",
    "    \n",
    "    plt.tick_params(axis='x', which='major', labelsize=8)\n",
    "    plt.tick_params(axis='y', which='major', labelsize=8)\n",
    "    \n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('')\n",
    "    plt.title('Distribution of Target in {}'.format(feature))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ssu8_DXgNeU"
   },
   "source": [
    "### **1.8 Conclusion**\n",
    "Data imbalance is very common in customer datasets like this. Oversampling **Class 1** or undersampling **Class 0** are suitable solutions for this dataset because of its large size. Since the dataset is big enough, resampling would not introduce underfitting.\n",
    "\n",
    "Training set has more unique values than test set so some part of test set is most likely synthetic. Rows with more frequent values are less reliable because test set has bumps over distribution peaks. This is also related to synthetic data in test set.\n",
    "\n",
    "Features are not correlated with each other or not dependent to each other. However, `target` feature has the highest correlation with `var_81` (**0.08**). This relationship can bu used to make other features more informative. If a feature is target encoded on `var_81`, it could give information about `target`.\n",
    "\n",
    "Values in **1st** and **4th** quartiles have higher chance to be **Class 1** than values in **2nd** and **3rd** quartile for almost every feature because of winsorization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8927875ac609ec1ec630853db51e35260bd1e530",
    "id": "xHMSZpSVgNeU"
   },
   "source": [
    "## **2. Feature Engineering and Data Augmentation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a36HDEBVgNeU"
   },
   "source": [
    "### **2.1 Separating Real/Synthetic Test Data and Magic Features**\n",
    "Using unique value count in a row to identify synthetic samples. If a row has at least one unique value in a feature, then it is real, otherwise it is synthetic. This technique is shared by [YaG320](https://www.kaggle.com/yag320) in this kernel [List of Fake Samples and Public/Private LB split](https://www.kaggle.com/yag320/list-of-fake-samples-and-public-private-lb-split) and it successfuly identifies synthetic samples in entire test set. This way the unusual bumps on the distribution peaks of test set features are captured. The magic features are extracted from the combination of training set and real samples in the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SeTeT0igNeU"
   },
   "outputs": [],
   "source": [
    "test = df_test.drop(['ID_code'], axis=1).values\n",
    "\n",
    "unique_count = np.zeros_like(test)\n",
    "\n",
    "for feature in range(test.shape[1]):\n",
    "    _, index, count = np.unique(test[:, feature], return_counts=True, return_index=True)\n",
    "    unique_count[index[count == 1], feature] += 1\n",
    "    \n",
    "real_samples = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\n",
    "synth_samples = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]\n",
    "\n",
    "print('Number of real samples in test set is {}'.format(len(real_samples)))\n",
    "print('Number of synthetic samples in test set is {}'.format(len(synth_samples)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pgy-B9FxgNeV"
   },
   "outputs": [],
   "source": [
    "features = [col for col in df_train.columns if col.startswith('var')]\n",
    "df_all = pd.concat([df_train, df_test.iloc[real_samples]])\n",
    "\n",
    "for feature in features:\n",
    "    temp = df_all[feature].value_counts(dropna=True)\n",
    "\n",
    "    df_train[feature + 'vc'] = df_train[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
    "    df_test[feature + 'vc'] = df_test[feature].map(temp).map(lambda x: min(10, x)).astype(np.uint8)\n",
    "\n",
    "    df_train[feature + 'sum'] = ((df_train[feature] - df_all[feature].mean()) * df_train[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32)\n",
    "    df_test[feature + 'sum'] = ((df_test[feature] - df_all[feature].mean()) * df_test[feature + 'vc'].map(lambda x: int(x > 1))).astype(np.float32) \n",
    "\n",
    "    df_train[feature + 'sum2'] = ((df_train[feature]) * df_train[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n",
    "    df_test[feature + 'sum2'] = ((df_test[feature]) * df_test[feature + 'vc'].map(lambda x: int(x > 2))).astype(np.float32)\n",
    "\n",
    "    df_train[feature + 'sum3'] = ((df_train[feature]) * df_train[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32) \n",
    "    df_test[feature + 'sum3'] = ((df_test[feature]) * df_test[feature + 'vc'].map(lambda x: int(x > 4))).astype(np.float32)\n",
    "    \n",
    "print('Training set shape after creating magic features: {}'.format(df_train.shape))\n",
    "print('Test set shape after creating magic features: {}'.format(df_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Px8dMmFpgNeV"
   },
   "source": [
    "### **2.2 Data Augmentation**\n",
    "Oversampling the data increases CV and LB score significantly since the data is imbalanced. This oversampling technique is shared by [Jiwei Liu](https://www.kaggle.com/jiweiliu) in this kernel [LGB 2 leaves + augment](https://www.kaggle.com/jiweiliu/lgb-2-leaves-augment)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ox0L0cSsgNeV"
   },
   "outputs": [],
   "source": [
    "def augment(x, y, t=2):\n",
    "    \n",
    "    xs, xn = [], []\n",
    "    \n",
    "    for i in range(t // 2):\n",
    "        mask = y == 0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        featnum = x1.shape[1] // 200 - 1\n",
    "\n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:, [c] + [200 + featnum * c + idc for idc in range(featnum)]] = x1[ids][:, [c] + [200 + featnum * c + idc for idc in range(featnum)]]\n",
    "        xn.append(x1)\n",
    "    \n",
    "    for i in range(t):\n",
    "        mask = y > 0\n",
    "        x1 = x[mask].copy()\n",
    "        ids = np.arange(x1.shape[0])\n",
    "        featnum = x1.shape[1] // 200 - 1\n",
    "        \n",
    "        for c in range(200):\n",
    "            np.random.shuffle(ids)\n",
    "            x1[:, [c] + [200 + featnum * c + idc for idc in range(1)]] = x1[ids][:, [c] + [200 + featnum * c + idc for idc in range(1)]]\n",
    "        xs.append(x1)\n",
    "\n",
    "    xs = np.vstack(xs)\n",
    "    xn = np.vstack(xn)\n",
    "    ys = np.ones(xs.shape[0])\n",
    "    yn = np.zeros(xn.shape[0])\n",
    "    x = np.vstack([x, xs, xn])\n",
    "    y = np.concatenate([y, ys, yn])\n",
    "    \n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ec0fd4e93266afc468380c8bea4053629295c1e6",
    "id": "j5n0vOgcgNeX"
   },
   "source": [
    "## **3. Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0fd4fa361e588db8d21a6bcf43b244e3307fc5c3",
    "id": "lNrx1VDGgNeX"
   },
   "source": [
    "### **3.1 LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e40551aefaadb7ac5bdae5d15b84a32a891ace2b",
    "id": "AH4-AdfzgNeX"
   },
   "outputs": [],
   "source": [
    "gbdt_param = {\n",
    "    # Core Parameters\n",
    "    'objective': 'binary',\n",
    "    'boosting': 'gbdt',\n",
    "    'learning_rate': 0.01,\n",
    "    'num_leaves': 15,\n",
    "    'tree_learner': 'serial',\n",
    "    'num_threads': 8,\n",
    "    'seed': SEED,\n",
    "    \n",
    "    # Learning Control Parameters\n",
    "    'max_depth': -1,\n",
    "    'min_data_in_leaf': 50,\n",
    "    'min_sum_hessian_in_leaf': 10,  \n",
    "    'bagging_fraction': 0.6,\n",
    "    'bagging_freq': 5,\n",
    "    'feature_fraction': 0.05,\n",
    "    'lambda_l1': 1.,\n",
    "    'bagging_seed': SEED,\n",
    "    \n",
    "    # Others\n",
    "    'verbosity ': 1,\n",
    "    'boost_from_average': False,\n",
    "    'metric': 'auc',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o51YOq0egNeX"
   },
   "outputs": [],
   "source": [
    "predictors = df_train.columns.tolist()[2:]\n",
    "X_test = df_test[predictors]\n",
    "\n",
    "n_splits = 5\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
    "\n",
    "oof = df_train[['ID_code', 'target']]\n",
    "oof['predict'] = 0\n",
    "predictions = df_test[['ID_code']]\n",
    "val_aucs = []\n",
    "feature_importance_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OaK6p8kpgNeY"
   },
   "outputs": [],
   "source": [
    "for fold, (train_ind, val_ind) in enumerate(skf.split(df_train, df_train.target.values)):\n",
    "    \n",
    "    X_train, y_train = df_train.iloc[train_ind][predictors], df_train.iloc[train_ind]['target']\n",
    "    X_valid, y_valid = df_train.iloc[val_ind][predictors], df_train.iloc[val_ind]['target']\n",
    "\n",
    "    N = 1\n",
    "    p_valid, yp = 0, 0\n",
    "        \n",
    "    for i in range(N):\n",
    "        print('\\nFold {} - N {}'.format(fold + 1, i + 1))\n",
    "        \n",
    "        X_t, y_t = augment(X_train.values, y_train.values)\n",
    "        weights = np.array([0.8] * X_t.shape[0])\n",
    "        weights[:X_train.shape[0]] = 1.0\n",
    "        print('Shape of X_train after augment: {}\\nShape of y_train after augment: {}'.format(X_t.shape, y_t.shape))\n",
    "        \n",
    "        X_t = pd.DataFrame(X_t)\n",
    "        X_t = X_t.add_prefix('var_')\n",
    "    \n",
    "        trn_data = lgb.Dataset(X_t, label=y_t, weight=weights)\n",
    "        val_data = lgb.Dataset(X_valid, label=y_valid)\n",
    "        evals_result = {}\n",
    "        \n",
    "        lgb_clf = lgb.train(gbdt_param, trn_data, 100000, valid_sets=[trn_data, val_data], early_stopping_rounds=5000, verbose_eval=1000, evals_result=evals_result)\n",
    "        p_valid += lgb_clf.predict(X_valid)\n",
    "        yp += lgb_clf.predict(X_test)\n",
    "        \n",
    "    fold_importance_df = pd.DataFrame()\n",
    "    fold_importance_df[\"feature\"] = predictors\n",
    "    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n",
    "    fold_importance_df[\"fold\"] = fold + 1\n",
    "    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "    \n",
    "    oof['predict'][val_ind] = p_valid / N\n",
    "    val_score = roc_auc_score(y_valid, p_valid)\n",
    "    val_aucs.append(val_score)\n",
    "    \n",
    "    predictions['fold{}'.format(fold + 1)] = yp / N\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1840010a5839ab91957622ab5b35b1478ac1c7bf",
    "id": "MDClpzOzgNeY"
   },
   "source": [
    "### **3.2 ROC-AUC Score**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "id": "MGhoXBmXgNeY"
   },
   "outputs": [],
   "source": [
    "mean_auc = np.mean(val_aucs)\n",
    "std_auc = np.std(val_aucs)\n",
    "all_auc = roc_auc_score(oof['target'], oof['predict'])\n",
    "\n",
    "print('Mean AUC: {}, std: {}.\\nAll AUC: {}.'.format(mean_auc, std_auc, all_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ByDnAtTgNeY"
   },
   "source": [
    "### **3.3 Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "75a2a98e755a97e723588130e85e7475efac9052",
    "id": "HPA9t-5YgNeY"
   },
   "outputs": [],
   "source": [
    "cols = (feature_importance_df[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:1000].index)\n",
    "best_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n",
    "\n",
    "plt.figure(figsize=(15, 150))\n",
    "sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "plt.title('LightGBM Features (Averaged over Folds)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c23036e206d7443716f02faad6043e56aaf6fe2",
    "id": "sTID6sZtgNeZ"
   },
   "source": [
    "### **3.4 Submission**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "06f2caf16969c8bad14fc9c77ac2bd446909ab36",
    "id": "yVNRBGeNgNeZ"
   },
   "outputs": [],
   "source": [
    "predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n",
    "predictions.to_csv('predictions.csv', index=None)\n",
    "sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n",
    "sub_df[\"target\"] = predictions['target']\n",
    "sub_df.to_csv(\"lgb_submission_best.csv\", index=False)\n",
    "oof.to_csv('lgb_oof.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BtMF_4rtg81b"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('lgb_submission_best.csv') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0O2Pio4wEOFZ"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download('lgb_submission_best.csv') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "santander_customer_transaction_eda_fe_lgb.ipynb",
   "private_outputs": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
